{
  "slug": "building-memory-for-writing",
  "title": "Building Memory for Writing",
  "date": "September 28, 2025",
  "excerpt": "I built a RAG system to query my own writing. What started as a weekend experiment became a mirror for understanding my own voice.",
  "category": "Writing",
  "firstImage": "https://www.nateking.dev/images/posts/mastra-document-ingestion.png",
  "content": "\nWriters accumulate words the way servers accumulate logs—relentlessly, continuously, without pause. Blog posts, essays, project documentation, half-finished drafts. Over years, it becomes a corpus. Over a career, it becomes overwhelming.\n\nI have dozens of blog posts scattered across Markdown files. Each one captured a thought, a project, a moment of clarity about some problem I was solving. But memory fades. I'd find myself asking: *Did I already write about this? What was that insight I had about AI-native design? Where did I talk about prompt engineering?*\n\nThe answer was always the same: dig through files, search manually, hope I remembered the right keywords. It felt archaic. We have better tools now. So I built one.\n\n## The Problem: Context Without Search\n\nTraditional search works when you know what you're looking for. You type keywords, you get matches, you scan results. It's deterministic, mechanical, and often frustrating when your memory is fuzzy.\n\nWhat I wanted was different: **semantic search over my own writing.** Not keyword matching, but meaning matching. I wanted to ask questions like *\"What have I written about RAG systems?\"* or *\"How did I explain the difference between features and foundations?\"* and get back not just documents, but synthesized answers drawn from everything I'd written.\n\nThis is the promise of Retrieval-Augmented Generation: give an AI access to your documents, let it find the relevant pieces, and have it answer questions with context it couldn't have learned during training. It's search that understands meaning, paired with generation that understands synthesis.\n\n## The Stack: Mastra, Voyage, and LibSQL\n\nI built this on [Mastra](https://mastra.ai), a TypeScript framework for AI agents and workflows. Mastra handles the orchestration—agents, tools, workflows, memory—letting me focus on the logic instead of the plumbing.\n\nFor embeddings, I used [Voyage AI's voyage-3-large model](https://www.nateking.dev/blog/voyage-context-3). Voyage specializes in high-quality embeddings, and voyage-3-large produces 1024-dimensional vectors that capture semantic meaning with impressive precision. I've been watching their work closely—contextualized embeddings are solving real problems in RAG systems, and their models consistently outperform alternatives in retrieval accuracy.\n\nFor storage, I went with LibSQL, a SQLite fork that Mastra already uses. No external dependencies, no database servers, just a local file that stores embeddings as binary blobs. Cosine similarity search happens in-memory. It's simple, fast, and eliminates infrastructure complexity.\n\nThe architecture is straightforward:\n\n1. **Document Ingestion Workflow**: Read Markdown files → chunk them into ~800 token segments with 200 token overlap → generate embeddings → store in vector database\n2. **Query Workflow**: User asks a question → embed the query → search for similar chunks → pass results to GPT-4o → synthesize an answer\n\nNo external APIs beyond OpenAI and Voyage. No authentication layers. No deployment complexity. Just a tool that works.\n\n## Implementation: Workflows, Not Scripts\n\nWhat makes Mastra elegant is its workflow model. Instead of writing scripts with manual orchestration, you define steps and chain them together. Each step has an input schema, an output schema, and an execute function. Mastra handles the rest.\n\nHere's the ingestion workflow:\n\n```typescript\nconst readDocuments = createStep({\n  id: 'read-documents',\n  execute: async ({ inputData }) => {\n\tconst result = await markdownReaderTool.execute({\n\t  context: { directoryPath: inputData.directoryPath }\n\t});\n\treturn result;\n  }\n});\n\nconst generateEmbeddings = createStep({\n  id: 'generate-embeddings',\n  execute: async ({ inputData }) => {\n\tconst { chunks } = inputData;\n\tconst texts = chunks.map(chunk => chunk.content);\n\n\tconst result = await voyageEmbeddingTool.execute({\n\t  context: { texts, inputType: 'document' }\n\t});\n\n\treturn { embeddings: result.embeddings, chunks };\n  }\n});\n\nconst storeEmbeddings = createStep({\n  id: 'store-embeddings',\n  execute: async ({ inputData }) => {\n\tawait vectorStoreTool.execute({\n\t  context: { chunks: inputData.embeddings }\n\t});\n\treturn { success: true };\n  }\n});\n\nexport const documentIngestionWorkflow = createWorkflow({\n  id: 'document-ingestion-workflow'\n})\n  .then(readDocuments)\n  .then(generateEmbeddings)\n  .then(storeEmbeddings);\n```\n\nThree steps, clear dependencies, automatic error handling. When I run this workflow, it processes every Markdown file in my `documents/` directory, generates embeddings in batches to stay under API rate limits, and stores everything in LibSQL. The entire corpus of my writing gets vectorized in a few minutes.\n\nThe query side is even simpler:\n\n```typescript\nconst processQuery = createStep({\n  id: 'process-query',\n  execute: async ({ inputData, mastra }) => {\n\tconst agent = mastra?.getAgent('writingAssistantAgent');\n\tconst response = await agent.stream([\n\t  { role: 'user', content: inputData.query }\n\t]);\n\n\t// Stream response to stdout\n\tfor await (const chunk of response.textStream) {\n\t  process.stdout.write(chunk);\n\t}\n  }\n});\n```\n\nThe agent has access to a `semanticSearchTool` that embeds the query, retrieves relevant chunks, and returns them as context. GPT-4o does the synthesis. The result feels conversational, but it's grounded in my actual writing.\n\n![](/images/posts/mastra-document-ingestion.png)\n\n## What I Learned: Context Is Everything\n\nBuilding this system reinforced something I already knew but hadn't fully internalized: **chunking strategy matters.**\n\nAt first, I tried simple splits—just break documents every 1000 characters. The results were terrible. Chunks cut off mid-sentence, context vanished, and retrieval became a game of luck. A query about \"AI-native design\" would return fragments that mentioned AI but missed the design philosophy entirely.\n\nThe fix was semantic chunking: split on natural boundaries (paragraphs, headings), keep chunks around 800 tokens, and overlap by 200 tokens so context flows between segments. This preserved narrative structure and dramatically improved retrieval quality.\n\nI also learned that **input\\_type matters** with Voyage embeddings. When generating embeddings for storage, you set `input_type: 'document'`. When embedding a query, you set `input_type: 'query'`. This subtle difference optimizes the vector space for asymmetric search—queries and documents occupy the same space but encode different information densities.\n\n## The Meta Moment: Asking It to Write This Post\n\nAfter building the system, I had a realization: *What if I asked it to write a blog post about itself?*\n\nSo I did. I ran the query workflow and typed: *\"Take a look at my blog posts and write a post about this project in my style.\"*\n\nThe system scanned my writing, identified patterns in how I structure posts, recognized my preference for direct technical explanations mixed with reflective commentary, and generated a draft. Not this post—I'm still the one writing—but a draft that captured the voice well enough to make me pause.\n\nIt's strange to see your own style reflected back at you. The system picked up on my tendency to open with a problem, my habit of using blockquotes for emphasis, my preference for concrete examples over abstract theory. It even nailed the tone: pragmatic, slightly wry, grounded in building actual things.\n\nThis is what AI-native tools should feel like: not replacing the work, but augmenting it. Not writing for you, but helping you see your own patterns more clearly.\n\n## What's Next: Memory as a Creative Tool\n\nRight now, this system is read-only. It ingests documents, answers questions, and that's it. But the next step is obvious: **make it bidirectional.**\n\nWhat if the writing assistant could track recurring themes across my work? What if it could suggest connections between ideas I've explored in different posts? What if it could identify gaps—topics I've touched on but never fully developed?\n\nMemory isn't just storage. It's context, continuity, and coherence. For writers, it's the difference between scattered thoughts and a body of work that builds on itself.\n\nAI gives us the tools to make that memory queryable, searchable, and generative. We're not just storing words anymore—we're building systems that understand them, connect them, and help us see what we've been saying all along.\n\n## The Craft of Building AI Tools\n\nThere's a particular satisfaction in building tools for yourself. No product requirements, no stakeholder meetings, no compromise between vision and feasibility. Just a problem you have, a tool you build, and the immediate feedback of using it daily.\n\nThis project took a weekend to build and another week to refine. The code is clean, the dependencies are minimal, and the system does exactly what I need. It's not a product. It's not a startup. It's a tool that makes my writing practice better.\n\nAnd that's enough.\n\n---\n\n*The writing-agent project is built with [Mastra](https://mastra.ai), [Voyage AI embeddings](https://voyageai.com), and LibSQL. All code lives in a single repository, runs locally, and costs pennies per query. It's open-source and can be found in [my GitHub repository](https://github.com/nathan-a-king/rag-writing-assistant).*"
}