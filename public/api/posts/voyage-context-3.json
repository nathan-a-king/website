{
  "slug": "voyage-context-3",
  "title": "Voyage Context 3: The Future of RAG?",
  "date": "August 3, 2025",
  "excerpt": "With Voyage's new voyage-context-3 model, developers can simplify their pipelines while improving answer quality across complex documents.",
  "category": "AI",
  "firstImage": null,
  "content": "\nI was immediately intrigued by [Voyage AI's recent blog article](https://blog.voyageai.com/2025/07/23/voyage-context-3/) introducing `voyage-context-3`, a new contextualized chunk embedding model. The model offers an elegant approach to resolving a core tension in Retrieval-Augmented Generation (RAG): balancing fine-grained precision from smaller chunks with the need for broader contextual understanding.\n\n## The Core Dilemma\n\n**Smaller chunks provide focused detail but lose global context.** When you break a large document into small segments, each chunk can capture very specific, granular information with high precision. However, these isolated chunks lack the broader document context that may be essential for understanding and answering queries accurately.\n\nSmaller chunks excel at capturing *precise, fine-grained information*. The announcement illustrates this with a legal contract example: if you have a 50-page document vectorized as a single embedding, specific details like \"All data transmissions between the Client and the Service Provider's infrastructure shall utilize AES-256 encryption in GCM mode\" would likely get buried or lost in the aggregate representation.\n\nBy chunking the document into paragraphs, the resulting embeddings can much better capture these localized technical details like \"AES-256 encryption,\" making retrieval more accurate for specific factual queries. However, that same precisely-chunked paragraph about AES-256 encryption may not contain global context like the client's name, which appears elsewhere in the document.\n\nTraditional workarounds to this issue include chunk overlapping and metadata augmentation, which introduce complexity, increase development time and marginal costs. Voyage AI's solution claims to provide focused detail and global context **without tradeoffs.** Let's take a look at how we can incorporate this into an application.\n\nLet's remove our old chunking code:\n\n```py\n# Standard embedding\nchunk_embds = vo.embed(\n    texts=[record[\"chunk\"] for record in all_chunks],\n    model=\"voyage-3-large\",\n    input_type=\"document\"\n).embeddings\n```\n\nAnd replace it with contextualized embeddings:\n\n```py\n# Contextualized embedding model\nquery_embd_context = vo.contextualized_embed(inputs=[[query]],\nmodel=\"voyage-context-3\", input_type=\"query\").results[0].embeddings[0]\n\nembds_obj = vo.contextualized_embed(\n    inputs=texts,\n    model=\"voyage-context-3\",\n    input_type=\"document\"\n)\n\ncontextualized_chunk_embds = [emb for r in embds_obj.results\nfor emb in r.embeddings]\n```\n\nThat's it! You can use your same semantic similarity search with contextualized chunk embeddings. Let's look at how it performs in real-world use.\n\nUsing [MikeRoyal's Apple Silicon Guide](https://github.com/mikeroyal/Apple-Silicon-Guide) as input,we posed a question that requires synthesizing information across multiple sections:\n\n`query = \"What are the differences between M1 Pro and M2 Max performance specs?\"`\n\nStandard embedding models return the following results:\n\n```text\nTo compare the performance specifications of the M1 Pro and M2 Max, \nwe can look at several key areas, including CPU performance, GPU performance, \nmemory bandwidth, and overall architecture enhancements.\n\n1. **CPU Performance**:\n   - The M1 Pro features up to 10 CPU cores, with a mix of high-performance \n     and high-efficiency cores. It is designed to handle demanding tasks\n     efficiently.\n   - The M2 Max, on the other hand, offers an enhanced architecture with up to \n     12 CPU cores, which provides improved performance for multi-threaded \n     applications and overall processing power.\n\n2. **GPU Performance**:\n   - The M1 Pro comes with a GPU that can have up to 16 cores, which is capable \n     of handling graphics-intensive tasks and providing good performance for \n     creative applications.\n   - The M2 Max significantly boosts this capability with up to 38 GPU cores, \n     offering a substantial increase in graphics performance, making it more \n     suitable for professional video editing, 3D rendering, and other \n     graphics-heavy workloads.\n\n3. **Memory Bandwidth**:\n   - The M1 Pro has a memory bandwidth of 200 GB/s, which is quite high and \n     supports efficient data transfer for demanding applications.\n   - The M2 Max increases this further, providing a memory bandwidth of 400 GB/s, \n     allowing for even faster data access and processing, which is crucial for \n     high-performance computing tasks.\n\n4. **Unified Memory**:\n   - The M1 Pro supports up to 32 GB of unified memory, which allows for \n     efficient memory usage across the CPU and GPU.\n   - In contrast, the M2 Max supports up to 96 GB of unified memory, enabling \n     it to handle larger datasets and more demanding applications without \n     performance bottlenecks.\n\n5. **Architecture Enhancements**:\n   - The M2 Max benefits from architectural improvements over the M1 Pro, \n     including better power efficiency and enhanced performance per watt. \n     This means that while the M2 Max is more powerful, it also manages \n     power consumption more effectively.\n\nIn summary, the M2 Max outperforms the M1 Pro in several critical areas: \nit has more CPU and GPU cores, higher memory bandwidth, greater unified \nmemory capacity, and overall architectural enhancements that contribute to \nbetter performance and efficiency. These improvements make the M2 Max \nparticularly well-suited for professional users who require top-tier \nperformance for demanding tasks.\n```\n\nWith voyage-context-3, we can get the following results:\n```text\nBased on the document, here are the key performance specification \ndifferences between the M1 Pro and M2 Max:\n\nM1 Pro Specifications:\n- CPU: 8 or 10-core system-on-a-chip (SoC)\n- GPU: 14-core or 16-core options\n- Memory: Up to 32GB of unified memory\n- Memory bandwidth: Up to 200GB/s\n- Target devices: MacBook Pro 14\"\n\nM2 Max Specifications:\n- CPU: 12-core (12 high-performance and 4 energy-efficient cores)\n- GPU: 38-core\n- Memory: Up to 96GB of unified memory\n- Memory bandwidth: 400GB/s\n- Neural Engine: 16-core\n- Target devices: MacBook Pro 14\", MacBook Pro 16\", and Mac mini\n\nKey Performance Differences:\nPerformance Improvements (M2 Max vs M1 Pro):\n- 20% faster CPU performance\n- 30% faster GPU performance\n- 40% faster Neural Engine performance\n\nCapacity Improvements:\n- 3x more maximum memory (96GB vs 32GB)\n- 2x higher memory bandwidth (400GB/s vs 200GB/s)\n- More than 2x GPU cores (38 vs 14-16 cores)\n- More CPU cores (12 vs 8-10 cores)\n\nThe M2 Max represents a significant step up from the M1 Pro, positioning \nit as Apple's most powerful chip in the M2-series lineup at the time of \nthis document, designed for the most demanding professional workloads \nrequiring substantial computational power and memory capacity.\n```\n\nThe result is a more accurate and contextually aware answer that seamlessly draws from multiple sections of the document. This is a powerful example of how contextualized embeddings can enhance the performance of RAG systems, providing both precision and broader context without the need for complex workarounds."
}